{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAR+ewVBhGj4iTfEVnqcKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreekar-Kandhadai/pyspark-interview-questions/blob/main/pyspark_date_function_questions_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKFJSRAWvyjg"
      },
      "outputs": [],
      "source": [
        "1. Extracting Year:\n",
        "Given a column event_date (format: yyyy-MM-dd),\n",
        "extract only the year for each row.\n",
        "Sample data:\n",
        "event_date: [\"2023-04-15\",\"2022-11-30\", \"2021-08-25\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[('2023-04-15',),('2022-11-30',), ('2021-08-25',)]\n",
        "\n",
        "schema=['event_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df=df.withColumn(\"event_date\",to_date(col('event_date'),'yyyy-MM-dd'))\n",
        "\n",
        "df1=df.withColumn(\"year\",year(col('event_date')))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trYeq6GDwDfG",
        "outputId": "eaa48f3f-69e1-4772-89c3-25cd194da264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-04-15|\n",
            "|2022-11-30|\n",
            "|2021-08-25|\n",
            "+----------+\n",
            "\n",
            "+----------+----+\n",
            "|event_date|year|\n",
            "+----------+----+\n",
            "|2023-04-15|2023|\n",
            "|2022-11-30|2022|\n",
            "|2021-08-25|2021|\n",
            "+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2.Date Difference Calculation:\n",
        "Calculate the difference (in days) between two date\n",
        "columns start_date and end_date.\n",
        "Sample data:\n",
        "start_date: [\"2023-01-01\",\"2023-03-15\"]\n",
        "end_date: [\"2023-02-01\",\"2023-03-20\"]\n"
      ],
      "metadata": {
        "id": "WzMoaJqm0S00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-01-01\",\"2023-02-01\"),(\"2023-03-15\",\"2023-03-20\")]\n",
        "\n",
        "schema=[\"start_date\",\"end_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"days\",datediff(col('end_date'),col('start_date')))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odWVNIn_0UII",
        "outputId": "a7ee1f1c-3200-4a43-b5f7-563032a44ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|start_date|  end_date|\n",
            "+----------+----------+\n",
            "|2023-01-01|2023-02-01|\n",
            "|2023-03-15|2023-03-20|\n",
            "+----------+----------+\n",
            "\n",
            "+----------+----------+----+\n",
            "|start_date|  end_date|days|\n",
            "+----------+----------+----+\n",
            "|2023-01-01|2023-02-01|  31|\n",
            "|2023-03-15|2023-03-20|   5|\n",
            "+----------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3. Filter Records Based on Date:\n",
        "Filter records where event_date is after 2023-06-01.\n",
        "Sample data:\n",
        "event_date: [\"2023-05-15\",\"2023-07-20\",\"2023-06-05\"]"
      ],
      "metadata": {
        "id": "eX6_aZbW2WFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-05-15\",),(\"2023-07-20\",),(\"2023-06-05\",)]\n",
        "\n",
        "schema=[\"event_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.filter(\"event_date > '2023-06-01'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPj8GP1c2YxR",
        "outputId": "cc6f7896-d798-4a37-d488-0fe4d4a20339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-05-15|\n",
            "|2023-07-20|\n",
            "|2023-06-05|\n",
            "+----------+\n",
            "\n",
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-07-20|\n",
            "|2023-06-05|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "4. Add 30 days to each date in the order_date column.\n",
        "Sample data:\n",
        "order_date: [\"2023-01-10\", \"2023-06-15\", \"2023-07-30\"]\n"
      ],
      "metadata": {
        "id": "GG0xThFs3cb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-01-10\",), (\"2023-06-15\",), (\"2023-07-30\",)]\n",
        "\n",
        "schema=['order_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn('new_date',date_add(col('order_date'),30))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l991SMUT3hA4",
        "outputId": "9ce49002-5930-42de-8161-e902b1c59c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|order_date|\n",
            "+----------+\n",
            "|2023-01-10|\n",
            "|2023-06-15|\n",
            "|2023-07-30|\n",
            "+----------+\n",
            "\n",
            "+----------+----------+\n",
            "|order_date|  new_date|\n",
            "+----------+----------+\n",
            "|2023-01-10|2023-02-09|\n",
            "|2023-06-15|2023-07-15|\n",
            "|2023-07-30|2023-08-29|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5. Find the Maximum Date:\n",
        "Determine the latest date from a column\n",
        "payment_date.\n",
        "Sample data:\n",
        "payment_date: [\"2023-02-15\", \"2023-06-25\", \"2023-01-10\"]"
      ],
      "metadata": {
        "id": "ogo1KnUR5S_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-02-15\",), (\"2023-06-25\",), (\"2023-01-10\",)]\n",
        "\n",
        "schema=['payment_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.select(max('payment_date')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1isg94c55ZMq",
        "outputId": "ead7aad4-25ae-4067-c100-7cc489a3a1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|payment_date|\n",
            "+------------+\n",
            "|  2023-02-15|\n",
            "|  2023-06-25|\n",
            "|  2023-01-10|\n",
            "+------------+\n",
            "\n",
            "+-----------------+\n",
            "|max(payment_date)|\n",
            "+-----------------+\n",
            "|       2023-06-25|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "6.Truncate Date to First Day of Month:\n",
        "Truncate the sale_date to the first day of its respective\n",
        "month.\n",
        "Sample data:\n",
        "sale_date: [\"2023-04-12\",\"2023-07-23\",\"2023-08-05\"]"
      ],
      "metadata": {
        "id": "koFHQGwG6qZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-04-12\",),(\"2023-07-23\",),(\"2023-08-05\",)]\n",
        "\n",
        "schema=[\"sale_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.withColumn(\"new_date\",trunc(col('sale_date'),\"month\")).show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MkOJDEA62U7",
        "outputId": "3987d5d0-ddd3-4912-a3a3-683c037683bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "| sale_date|\n",
            "+----------+\n",
            "|2023-04-12|\n",
            "|2023-07-23|\n",
            "|2023-08-05|\n",
            "+----------+\n",
            "\n",
            "+----------+----------+\n",
            "| sale_date|  new_date|\n",
            "+----------+----------+\n",
            "|2023-04-12|2023-04-01|\n",
            "|2023-07-23|2023-07-01|\n",
            "|2023-08-05|2023-08-01|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7. Group by Year:\n",
        "Group records by year extracted from the column\n",
        "transaction_date.\n",
        "Sample data:\n",
        "transaction_date: [\"2023-06-12\", \"2022-11-09\",\"2021-04-01\"]"
      ],
      "metadata": {
        "id": "gkfZ60r88DNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"2023-06-12\",), (\"2022-11-09\",),(\"2021-04-01\",)]\n",
        "\n",
        "schema=['transaction_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"year\",year(col('transaction_date'))).groupBy('year').agg(count(\"*\").alias('count'))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lpUQ4ib8U7H",
        "outputId": "c16d6882-fb57-4c6b-9f57-d481024d1713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|transaction_date|\n",
            "+----------------+\n",
            "|      2023-06-12|\n",
            "|      2022-11-09|\n",
            "|      2021-04-01|\n",
            "+----------------+\n",
            "\n",
            "+----+-----+\n",
            "|year|count|\n",
            "+----+-----+\n",
            "|2023|    1|\n",
            "|2022|    1|\n",
            "|2021|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8. Filter Records Within a Date Range:\n",
        "Filter records where visit_date is between 2023-01-01 and 2023-05-01.\n",
        "Sample data:\n",
        "visit_date: [\"2023-02-15\",\"2023-06-01\", \"2023-03-20\"]"
      ],
      "metadata": {
        "id": "PpU65F8F9YAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-02-15\",),(\"2023-06-01\",), (\"2023-03-20\",)]\n",
        "\n",
        "schema=['visit_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.filter(col('visit_date').between('2023-01-01','2023-05-01')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdpZ7L5I9m3Q",
        "outputId": "5bb82f24-501a-4efc-de45-ba70a9e1bc44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|visit_date|\n",
            "+----------+\n",
            "|2023-02-15|\n",
            "|2023-06-01|\n",
            "|2023-03-20|\n",
            "+----------+\n",
            "\n",
            "+----------+\n",
            "|visit_date|\n",
            "+----------+\n",
            "|2023-02-15|\n",
            "|2023-03-20|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "9. Day of Week Extraction:\n",
        "Extract the day of the week from attendance_date.\n",
        "Sample data:\n",
        "attendance_date: [\"2023-08-11\", \"2023-07-25\", \"2023-09-01\"]"
      ],
      "metadata": {
        "id": "JK9z7Nz1DjFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"2023-08-11\",), (\"2023-07-25\",), (\"2023-09-01\",)]\n",
        "\n",
        "schema=['attendance_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn('day_of_week_num', dayofweek(col('attendance_date')))\\\n",
        "      .withColumn('day_of_week_name',date_format(col('attendance_date'),'EEEE'))\n",
        "\n",
        "df1.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVOgXr-ZDnT7",
        "outputId": "830614dd-49e6-457f-df38-029fd9b42906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|attendance_date|\n",
            "+---------------+\n",
            "|     2023-08-11|\n",
            "|     2023-07-25|\n",
            "|     2023-09-01|\n",
            "+---------------+\n",
            "\n",
            "+---------------+---------------+----------------+\n",
            "|attendance_date|day_of_week_num|day_of_week_name|\n",
            "+---------------+---------------+----------------+\n",
            "|     2023-08-11|              6|          Friday|\n",
            "|     2023-07-25|              3|         Tuesday|\n",
            "|     2023-09-01|              6|          Friday|\n",
            "+---------------+---------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10. Check Leap Year:\n",
        "Identify if each date in birth_date falls in a leap year.\n",
        "Sample data:\n",
        "birth_date: [\"2020-03-01\",\"2019-12-15\", \"2024-02-29\"]\n"
      ],
      "metadata": {
        "id": "MRwj3XklFPqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"2020-03-01\",),(\"2019-12-15\",), (\"2024-02-29\",)]\n",
        "\n",
        "schema=['birth_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"leap_year\", when(((year(col('birth_date'))%4==0) & ((year(col('birth_date'))%400==0) | (year(col('birth_date'))%100!=0) )) ,lit('yes') ).otherwise('No'))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haXvLVOkFTPO",
        "outputId": "763ba0fd-0c64-4c4e-87a3-a97abcd1313a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|birth_date|\n",
            "+----------+\n",
            "|2020-03-01|\n",
            "|2019-12-15|\n",
            "|2024-02-29|\n",
            "+----------+\n",
            "\n",
            "+----------+---------+\n",
            "|birth_date|leap_year|\n",
            "+----------+---------+\n",
            "|2020-03-01|      yes|\n",
            "|2019-12-15|       No|\n",
            "|2024-02-29|      yes|\n",
            "+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "11.Convert String to Date:\n",
        "Convert a string column arrival_time (format: ddMM-yyyy) to date format.\n",
        "Sample data:\n",
        "arrival_time: [\"15-04-2023\", \"20-08-2023\", \"01-12-2023\"]"
      ],
      "metadata": {
        "id": "70ADmflpNJQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"15-04-2023\",), (\"20-08-2023\",), (\"01-12-2023\",)]\n",
        "\n",
        "schema=[\"arrival_time\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df1=df.withColumn(\"arrival_time\",to_date(col(\"arrival_time\"),'dd-MM-yyy'))\n",
        "\n",
        "df1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv8BltzUNMcD",
        "outputId": "f7d2fb88-48d0-4f66-b2fb-78dcfece7425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|arrival_time|\n",
            "+------------+\n",
            "|  15-04-2023|\n",
            "|  20-08-2023|\n",
            "|  01-12-2023|\n",
            "+------------+\n",
            "\n",
            "root\n",
            " |-- arrival_time: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- arrival_time: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "12. Calculate Week Number:\n",
        "For each date in shipment_date, calculate the week\n",
        "number of the year.\n",
        "Sample data:\n",
        "shipment_date: [\"2023-02-15\", \"2023-08-01\", \"2023-12-25\"]"
      ],
      "metadata": {
        "id": "RaNd8cn3OXBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data= [(\"2023-02-15\",), (\"2023-08-01\",), (\"2023-12-25\",)]\n",
        "\n",
        "schema=[\"shipment_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"week_number\",weekofyear(col('shipment_date')))\n",
        "\n",
        "df1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCTCE8LbOkMs",
        "outputId": "7e274f4f-8d9f-444c-aa03-8a941820cc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|shipment_date|\n",
            "+-------------+\n",
            "|   2023-02-15|\n",
            "|   2023-08-01|\n",
            "|   2023-12-25|\n",
            "+-------------+\n",
            "\n",
            "+-------------+-----------+\n",
            "|shipment_date|week_number|\n",
            "+-------------+-----------+\n",
            "|   2023-02-15|          7|\n",
            "|   2023-08-01|         31|\n",
            "|   2023-12-25|         52|\n",
            "+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "13. Find Records from the Last 7 Days:\n",
        "Identify all records where log_date is within the last\n",
        "7 days from the current date.\n",
        "Sample data:\n",
        "log_date: [\"2023-08-08\",\"2023-08-11\", \"2023-08-15\"]"
      ],
      "metadata": {
        "id": "hEqeemEDPzDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"2025-02-17\",),(\"2025-02-16\",), (\"2025-02-07\",)]\n",
        "\n",
        "schema=[\"log_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.filter(datediff(current_date(),col('log_date'))<=7)\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTJexRD3P2pF",
        "outputId": "7e5af0ee-c6d8-4d0c-e764-b3610a139380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|  log_date|\n",
            "+----------+\n",
            "|2025-02-17|\n",
            "|2025-02-16|\n",
            "|2025-02-07|\n",
            "+----------+\n",
            "\n",
            "+----------+\n",
            "|  log_date|\n",
            "+----------+\n",
            "|2025-02-17|\n",
            "|2025-02-16|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "14. Format Date as String:\n",
        "Format the booking_date as dd/MM/yyyy.\n",
        "Sample data:\n",
        "booking_date: [\"2023-07-12\", \"2023-09-15\", \"2023-05-30\"]\n"
      ],
      "metadata": {
        "id": "K3YEcNW2RTdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-07-12\",), (\"2023-09-15\",), (\"2023-05-30\",)]\n",
        "\n",
        "schema=[\"booking_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"booking_date\",date_format(col(\"booking_date\"),'dd/MM/yyyy'))\\\n",
        "      .withColumn(\"booking_date\",col(\"booking_date\").cast(\"string\"))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNhph8GdR98o",
        "outputId": "813e4477-859b-4166-b413-0309fb0d824f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|booking_date|\n",
            "+------------+\n",
            "|  2023-07-12|\n",
            "|  2023-09-15|\n",
            "|  2023-05-30|\n",
            "+------------+\n",
            "\n",
            "+------------+\n",
            "|booking_date|\n",
            "+------------+\n",
            "|  12/07/2023|\n",
            "|  15/09/2023|\n",
            "|  30/05/2023|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "15. Find the First and Last Record by Date:\n",
        "Find the first and last record based on the created_at\n",
        "date.\n",
        "Sample data:\n",
        "created_at: [\"2023-02-10\", \"2023-07-01\", \"2023-03-15\"]"
      ],
      "metadata": {
        "id": "jmQMuOmsTh3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "data = [(\"2023-02-10\",), (\"2023-07-01\",), (\"2023-03-15\",)]\n",
        "columns = [\"created_at\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df = df.withColumn(\"created_at\", col(\"created_at\").cast(\"date\"))\n",
        "\n",
        "first_record = df.orderBy(col(\"created_at\").asc()).limit(1)\n",
        "last_record = df.orderBy(col(\"created_at\").desc()).limit(1)\n",
        "\n",
        "first_record.show()\n",
        "last_record.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As7NfE1kTjP3",
        "outputId": "69ddf15c-7489-4ed4-ef53-4719911c610c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|created_at|\n",
            "+----------+\n",
            "|2023-02-10|\n",
            "+----------+\n",
            "\n",
            "+----------+\n",
            "|created_at|\n",
            "+----------+\n",
            "|2023-07-01|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "16. Difference Between Dates in Months:\n",
        " Calculate the difference between two dates in\n",
        " months (start_month and end_month).\n",
        " Sample data:\n",
        " start_month: [\"2023-01-01\", \"2023-04-01\"]\n",
        " end_month: [\"2023-06-01\", \"2023-07-01\"]"
      ],
      "metadata": {
        "id": "T4QdtRYfoePa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-01-01\",\"2023-06-01\"),(\"2023-04-01\",\"2023-07-01\")]\n",
        "\n",
        "schema=[\"start_date\",\"end_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"months\",months_between(col('end_date'),col('start_date')) )\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cZZUgt2pLYw",
        "outputId": "d5c305e2-489c-4bc7-84ae-337d5b46cbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|start_date|  end_date|\n",
            "+----------+----------+\n",
            "|2023-01-01|2023-06-01|\n",
            "|2023-04-01|2023-07-01|\n",
            "+----------+----------+\n",
            "\n",
            "+----------+----------+------+\n",
            "|start_date|  end_date|months|\n",
            "+----------+----------+------+\n",
            "|2023-01-01|2023-06-01|   5.0|\n",
            "|2023-04-01|2023-07-01|   3.0|\n",
            "+----------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " 17. Convert UTC to Local Time:\n",
        " Convert the utc_timestamp column from UTC to a\n",
        " local timezone (e.g., IST).\n",
        " Sample data:\n",
        " utc_timestamp: [\"2023-08-14T12:00:00Z\", \"2023-08-14T18:30:00Z\"]"
      ],
      "metadata": {
        "id": "8MA3wI1nsrHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-08-14T12:00:00Z\",), (\"2023-08-14T18:30:00Z\",)]\n",
        "\n",
        "schema=['utc_timestamp']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"utc_timestamp\", col('utc_timestamp').cast(\"timestamp\"))\n",
        "\n",
        "df1=df1.withColumn(\"ist_timestamp\",from_utc_timestamp(col('utc_timestamp'),\"Asia/Kolkata\"))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoXHj8aAuTBb",
        "outputId": "b297eb51-b7be-4c84-da3f-5f08a5f021e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|       utc_timestamp|\n",
            "+--------------------+\n",
            "|2023-08-14T12:00:00Z|\n",
            "|2023-08-14T18:30:00Z|\n",
            "+--------------------+\n",
            "\n",
            "+-------------------+-------------------+\n",
            "|      utc_timestamp|      ist_timestamp|\n",
            "+-------------------+-------------------+\n",
            "|2023-08-14 12:00:00|2023-08-14 17:30:00|\n",
            "|2023-08-14 18:30:00|2023-08-15 00:00:00|\n",
            "+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "18. Find Holidays:\n",
        " Check if dates in the holiday_date column are public\n",
        " holidays (e.g., 2023-01-01, 2023-12-25).\n",
        " Sample data:\n",
        " holiday_date: [\"2023-01-01\", \"2023-11-24\", \"2023-12-25\"]"
      ],
      "metadata": {
        "id": "rm7GJ0gVwfa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-01-01\",), (\"2023-11-24\",), (\"2023-12-25\",)]\n",
        "\n",
        "schema=['holiday_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "public_holiday=['2023-01-01','2023-12-25']\n",
        "\n",
        "df1=df.withColumn(\"is_holiday\",when(col('holiday_date').isin(public_holiday),\"Yes\").otherwise(\"No\"))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIXNmy7gwkMk",
        "outputId": "e3471758-133f-4f38-ea60-b462a772bd61"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|holiday_date|\n",
            "+------------+\n",
            "|  2023-01-01|\n",
            "|  2023-11-24|\n",
            "|  2023-12-25|\n",
            "+------------+\n",
            "\n",
            "+------------+----------+\n",
            "|holiday_date|is_holiday|\n",
            "+------------+----------+\n",
            "|  2023-01-01|       Yes|\n",
            "|  2023-11-24|        No|\n",
            "|  2023-12-25|       Yes|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "19. Round Time to Nearest Hour:\n",
        " Round the meeting_time column to the nearest hour.\n",
        " Sample data:\n",
        " meeting_time: [\"2023-08-14T12:35:00\", \"2023-08-14T18:25:00\"]\n",
        ""
      ],
      "metadata": {
        "id": "_XbDq0HDyX4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, round\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RoundTime\").getOrCreate()\n",
        "\n",
        "data = [(\"2023-08-14T12:35:00\",), (\"2023-08-14T18:25:00\",)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"meeting_time\"])\n",
        "\n",
        "df.show()\n",
        "\n",
        "df = df.withColumn(\"meeting_time\", col(\"meeting_time\").cast(\"timestamp\"))\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Round to the nearest hour\n",
        "df = df.withColumn(\n",
        "    \"rounded_time\",\n",
        "    from_unixtime(round(unix_timestamp(col(\"meeting_time\")) / 3600) * 3600, \"yyyy-MM-dd HH:mm:ss\")\n",
        ")\n",
        "\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "df_test= df.withColumn(\n",
        "    \"rounded_time\",round(unix_timestamp(col(\"meeting_time\")) / 3600) * 3600)\n",
        "\n",
        "\n",
        "df_test.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clrjmJL5yro0",
        "outputId": "832e6ed1-7da9-4aa6-8169-f729f530a120"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|       meeting_time|\n",
            "+-------------------+\n",
            "|2023-08-14T12:35:00|\n",
            "|2023-08-14T18:25:00|\n",
            "+-------------------+\n",
            "\n",
            "+-------------------+\n",
            "|       meeting_time|\n",
            "+-------------------+\n",
            "|2023-08-14 12:35:00|\n",
            "|2023-08-14 18:25:00|\n",
            "+-------------------+\n",
            "\n",
            "+-------------------+-------------------+\n",
            "|meeting_time       |rounded_time       |\n",
            "+-------------------+-------------------+\n",
            "|2023-08-14 12:35:00|2023-08-14 13:00:00|\n",
            "|2023-08-14 18:25:00|2023-08-14 18:00:00|\n",
            "+-------------------+-------------------+\n",
            "\n",
            "+-------------------+------------+\n",
            "|meeting_time       |rounded_time|\n",
            "+-------------------+------------+\n",
            "|2023-08-14 12:35:00|1.692018E9  |\n",
            "|2023-08-14 18:25:00|1.692036E9  |\n",
            "+-------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " 20. Extract Quarter:\n",
        " Extract the quarter of the year from the\n",
        " invoice_date.\n",
        " Sample data:\n",
        " invoice_date: [\"2023-03-15\", \"2023-06-01\", \"2023-10-20\"]"
      ],
      "metadata": {
        "id": "qX8VXZxG1xD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-03-15\",), (\"2023-06-01\",), (\"2023-10-20\",)]\n",
        "\n",
        "schema=['invoice_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"quarter\",quarter(col('invoice_date')))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1u1Ykg12PKt",
        "outputId": "339587f4-e4fd-409a-c1cb-5e846e224937"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|invoice_date|\n",
            "+------------+\n",
            "|  2023-03-15|\n",
            "|  2023-06-01|\n",
            "|  2023-10-20|\n",
            "+------------+\n",
            "\n",
            "+------------+-------+\n",
            "|invoice_date|quarter|\n",
            "+------------+-------+\n",
            "|  2023-03-15|      1|\n",
            "|  2023-06-01|      2|\n",
            "|  2023-10-20|      4|\n",
            "+------------+-------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}