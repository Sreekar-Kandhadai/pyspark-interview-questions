{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMePLYrfQAOfjEimRg7huaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreekar-Kandhadai/pyspark-interview-questions/blob/main/Pyspark_Date_Function_Questions_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKFJSRAWvyjg"
      },
      "outputs": [],
      "source": [
        "1. Extracting Year:\n",
        "Given a column event_date (format: yyyy-MM-dd),\n",
        "extract only the year for each row.\n",
        "Sample data:\n",
        "event_date: [\"2023-04-15\",\"2022-11-30\", \"2021-08-25\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[('2023-04-15',),('2022-11-30',), ('2021-08-25',)]\n",
        "\n",
        "schema=['event_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df=df.withColumn(\"event_date\",to_date(col('event_date'),'yyyy-MM-dd'))\n",
        "\n",
        "df1=df.withColumn(\"year\",year(col('event_date')))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trYeq6GDwDfG",
        "outputId": "eaa48f3f-69e1-4772-89c3-25cd194da264"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-04-15|\n",
            "|2022-11-30|\n",
            "|2021-08-25|\n",
            "+----------+\n",
            "\n",
            "+----------+----+\n",
            "|event_date|year|\n",
            "+----------+----+\n",
            "|2023-04-15|2023|\n",
            "|2022-11-30|2022|\n",
            "|2021-08-25|2021|\n",
            "+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2.Date Difference Calculation:\n",
        "Calculate the difference (in days) between two date\n",
        "columns start_date and end_date.\n",
        "Sample data:\n",
        "start_date: [\"2023-01-01\",\"2023-03-15\"]\n",
        "end_date: [\"2023-02-01\",\"2023-03-20\"]\n"
      ],
      "metadata": {
        "id": "WzMoaJqm0S00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-01-01\",\"2023-02-01\"),(\"2023-03-15\",\"2023-03-20\")]\n",
        "\n",
        "schema=[\"start_date\",\"end_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"days\",datediff(col('end_date'),col('start_date')))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odWVNIn_0UII",
        "outputId": "a7ee1f1c-3200-4a43-b5f7-563032a44ea5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|start_date|  end_date|\n",
            "+----------+----------+\n",
            "|2023-01-01|2023-02-01|\n",
            "|2023-03-15|2023-03-20|\n",
            "+----------+----------+\n",
            "\n",
            "+----------+----------+----+\n",
            "|start_date|  end_date|days|\n",
            "+----------+----------+----+\n",
            "|2023-01-01|2023-02-01|  31|\n",
            "|2023-03-15|2023-03-20|   5|\n",
            "+----------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3. Filter Records Based on Date:\n",
        "Filter records where event_date is after 2023-06-01.\n",
        "Sample data:\n",
        "event_date: [\"2023-05-15\",\"2023-07-20\",\"2023-06-05\"]"
      ],
      "metadata": {
        "id": "eX6_aZbW2WFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-05-15\",),(\"2023-07-20\",),(\"2023-06-05\",)]\n",
        "\n",
        "schema=[\"event_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.filter(\"event_date > '2023-06-01'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPj8GP1c2YxR",
        "outputId": "cc6f7896-d798-4a37-d488-0fe4d4a20339"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-05-15|\n",
            "|2023-07-20|\n",
            "|2023-06-05|\n",
            "+----------+\n",
            "\n",
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-07-20|\n",
            "|2023-06-05|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "4. Add 30 days to each date in the order_date column.\n",
        "Sample data:\n",
        "order_date: [\"2023-01-10\", \"2023-06-15\", \"2023-07-30\"]\n"
      ],
      "metadata": {
        "id": "GG0xThFs3cb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-01-10\",), (\"2023-06-15\",), (\"2023-07-30\",)]\n",
        "\n",
        "schema=['order_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn('new_date',date_add(col('order_date'),30))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l991SMUT3hA4",
        "outputId": "9ce49002-5930-42de-8161-e902b1c59c4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|order_date|\n",
            "+----------+\n",
            "|2023-01-10|\n",
            "|2023-06-15|\n",
            "|2023-07-30|\n",
            "+----------+\n",
            "\n",
            "+----------+----------+\n",
            "|order_date|  new_date|\n",
            "+----------+----------+\n",
            "|2023-01-10|2023-02-09|\n",
            "|2023-06-15|2023-07-15|\n",
            "|2023-07-30|2023-08-29|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5. Find the Maximum Date:\n",
        "Determine the latest date from a column\n",
        "payment_date.\n",
        "Sample data:\n",
        "payment_date: [\"2023-02-15\", \"2023-06-25\", \"2023-01-10\"]"
      ],
      "metadata": {
        "id": "ogo1KnUR5S_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-02-15\",), (\"2023-06-25\",), (\"2023-01-10\",)]\n",
        "\n",
        "schema=['payment_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.select(max('payment_date')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1isg94c55ZMq",
        "outputId": "ead7aad4-25ae-4067-c100-7cc489a3a1ab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|payment_date|\n",
            "+------------+\n",
            "|  2023-02-15|\n",
            "|  2023-06-25|\n",
            "|  2023-01-10|\n",
            "+------------+\n",
            "\n",
            "+-----------------+\n",
            "|max(payment_date)|\n",
            "+-----------------+\n",
            "|       2023-06-25|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "6.Truncate Date to First Day of Month:\n",
        "Truncate the sale_date to the first day of its respective\n",
        "month.\n",
        "Sample data:\n",
        "sale_date: [\"2023-04-12\",\"2023-07-23\",\"2023-08-05\"]"
      ],
      "metadata": {
        "id": "koFHQGwG6qZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-04-12\",),(\"2023-07-23\",),(\"2023-08-05\",)]\n",
        "\n",
        "schema=[\"sale_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.withColumn(\"new_date\",trunc(col('sale_date'),\"month\")).show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MkOJDEA62U7",
        "outputId": "3987d5d0-ddd3-4912-a3a3-683c037683bb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "| sale_date|\n",
            "+----------+\n",
            "|2023-04-12|\n",
            "|2023-07-23|\n",
            "|2023-08-05|\n",
            "+----------+\n",
            "\n",
            "+----------+----------+\n",
            "| sale_date|  new_date|\n",
            "+----------+----------+\n",
            "|2023-04-12|2023-04-01|\n",
            "|2023-07-23|2023-07-01|\n",
            "|2023-08-05|2023-08-01|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7. Group by Year:\n",
        "Group records by year extracted from the column\n",
        "transaction_date.\n",
        "Sample data:\n",
        "transaction_date: [\"2023-06-12\", \"2022-11-09\",\"2021-04-01\"]"
      ],
      "metadata": {
        "id": "gkfZ60r88DNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"2023-06-12\",), (\"2022-11-09\",),(\"2021-04-01\",)]\n",
        "\n",
        "schema=['transaction_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"year\",year(col('transaction_date'))).groupBy('year').agg(count(\"*\").alias('count'))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lpUQ4ib8U7H",
        "outputId": "c16d6882-fb57-4c6b-9f57-d481024d1713"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|transaction_date|\n",
            "+----------------+\n",
            "|      2023-06-12|\n",
            "|      2022-11-09|\n",
            "|      2021-04-01|\n",
            "+----------------+\n",
            "\n",
            "+----+-----+\n",
            "|year|count|\n",
            "+----+-----+\n",
            "|2023|    1|\n",
            "|2022|    1|\n",
            "|2021|    1|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8. Filter Records Within a Date Range:\n",
        "Filter records where visit_date is between 2023-01-01 and 2023-05-01.\n",
        "Sample data:\n",
        "visit_date: [\"2023-02-15\",\"2023-06-01\", \"2023-03-20\"]"
      ],
      "metadata": {
        "id": "PpU65F8F9YAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-02-15\",),(\"2023-06-01\",), (\"2023-03-20\",)]\n",
        "\n",
        "schema=['visit_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.filter(col('visit_date').between('2023-01-01','2023-05-01')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdpZ7L5I9m3Q",
        "outputId": "5bb82f24-501a-4efc-de45-ba70a9e1bc44"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|visit_date|\n",
            "+----------+\n",
            "|2023-02-15|\n",
            "|2023-06-01|\n",
            "|2023-03-20|\n",
            "+----------+\n",
            "\n",
            "+----------+\n",
            "|visit_date|\n",
            "+----------+\n",
            "|2023-02-15|\n",
            "|2023-03-20|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "9. Day of Week Extraction:\n",
        "Extract the day of the week from attendance_date.\n",
        "Sample data:\n",
        "attendance_date: [\"2023-08-11\", \"2023-07-25\", \"2023-09-01\"]"
      ],
      "metadata": {
        "id": "JK9z7Nz1DjFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"2023-08-11\",), (\"2023-07-25\",), (\"2023-09-01\",)]\n",
        "\n",
        "schema=['attendance_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn('day_of_week_num', dayofweek(col('attendance_date')))\\\n",
        "      .withColumn('day_of_week_name',date_format(col('attendance_date'),'EEEE'))\n",
        "\n",
        "df1.show()\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVOgXr-ZDnT7",
        "outputId": "830614dd-49e6-457f-df38-029fd9b42906"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|attendance_date|\n",
            "+---------------+\n",
            "|     2023-08-11|\n",
            "|     2023-07-25|\n",
            "|     2023-09-01|\n",
            "+---------------+\n",
            "\n",
            "+---------------+---------------+----------------+\n",
            "|attendance_date|day_of_week_num|day_of_week_name|\n",
            "+---------------+---------------+----------------+\n",
            "|     2023-08-11|              6|          Friday|\n",
            "|     2023-07-25|              3|         Tuesday|\n",
            "|     2023-09-01|              6|          Friday|\n",
            "+---------------+---------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10. Check Leap Year:\n",
        "Identify if each date in birth_date falls in a leap year.\n",
        "Sample data:\n",
        "birth_date: [\"2020-03-01\",\"2019-12-15\", \"2024-02-29\"]\n"
      ],
      "metadata": {
        "id": "MRwj3XklFPqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"2020-03-01\",),(\"2019-12-15\",), (\"2024-02-29\",)]\n",
        "\n",
        "schema=['birth_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"leap_year\", when(((year(col('birth_date'))%4==0) & ((year(col('birth_date'))%400==0) | (year(col('birth_date'))%100!=0) )) ,lit('yes') ).otherwise('No'))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haXvLVOkFTPO",
        "outputId": "763ba0fd-0c64-4c4e-87a3-a97abcd1313a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|birth_date|\n",
            "+----------+\n",
            "|2020-03-01|\n",
            "|2019-12-15|\n",
            "|2024-02-29|\n",
            "+----------+\n",
            "\n",
            "+----------+---------+\n",
            "|birth_date|leap_year|\n",
            "+----------+---------+\n",
            "|2020-03-01|      yes|\n",
            "|2019-12-15|       No|\n",
            "|2024-02-29|      yes|\n",
            "+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "11.Convert String to Date:\n",
        "Convert a string column arrival_time (format: ddMM-yyyy) to date format.\n",
        "Sample data:\n",
        "arrival_time: [\"15-04-2023\", \"20-08-2023\", \"01-12-2023\"]"
      ],
      "metadata": {
        "id": "70ADmflpNJQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"15-04-2023\",), (\"20-08-2023\",), (\"01-12-2023\",)]\n",
        "\n",
        "schema=[\"arrival_time\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df1=df.withColumn(\"arrival_time\",to_date(col(\"arrival_time\"),'dd-MM-yyy'))\n",
        "\n",
        "df1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv8BltzUNMcD",
        "outputId": "f7d2fb88-48d0-4f66-b2fb-78dcfece7425"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|arrival_time|\n",
            "+------------+\n",
            "|  15-04-2023|\n",
            "|  20-08-2023|\n",
            "|  01-12-2023|\n",
            "+------------+\n",
            "\n",
            "root\n",
            " |-- arrival_time: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- arrival_time: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "12. Calculate Week Number:\n",
        "For each date in shipment_date, calculate the week\n",
        "number of the year.\n",
        "Sample data:\n",
        "shipment_date: [\"2023-02-15\", \"2023-08-01\", \"2023-12-25\"]"
      ],
      "metadata": {
        "id": "RaNd8cn3OXBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data= [(\"2023-02-15\",), (\"2023-08-01\",), (\"2023-12-25\",)]\n",
        "\n",
        "schema=[\"shipment_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"week_number\",weekofyear(col('shipment_date')))\n",
        "\n",
        "df1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCTCE8LbOkMs",
        "outputId": "7e274f4f-8d9f-444c-aa03-8a941820cc0c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|shipment_date|\n",
            "+-------------+\n",
            "|   2023-02-15|\n",
            "|   2023-08-01|\n",
            "|   2023-12-25|\n",
            "+-------------+\n",
            "\n",
            "+-------------+-----------+\n",
            "|shipment_date|week_number|\n",
            "+-------------+-----------+\n",
            "|   2023-02-15|          7|\n",
            "|   2023-08-01|         31|\n",
            "|   2023-12-25|         52|\n",
            "+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "13. Find Records from the Last 7 Days:\n",
        "Identify all records where log_date is within the last\n",
        "7 days from the current date.\n",
        "Sample data:\n",
        "log_date: [\"2023-08-08\",\"2023-08-11\", \"2023-08-15\"]"
      ],
      "metadata": {
        "id": "hEqeemEDPzDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data=[(\"2025-02-17\",),(\"2025-02-16\",), (\"2025-02-07\",)]\n",
        "\n",
        "schema=[\"log_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.filter(datediff(current_date(),col('log_date'))<=7)\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTJexRD3P2pF",
        "outputId": "7e5af0ee-c6d8-4d0c-e764-b3610a139380"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|  log_date|\n",
            "+----------+\n",
            "|2025-02-17|\n",
            "|2025-02-16|\n",
            "|2025-02-07|\n",
            "+----------+\n",
            "\n",
            "+----------+\n",
            "|  log_date|\n",
            "+----------+\n",
            "|2025-02-17|\n",
            "|2025-02-16|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "14. Format Date as String:\n",
        "Format the booking_date as dd/MM/yyyy.\n",
        "Sample data:\n",
        "booking_date: [\"2023-07-12\", \"2023-09-15\", \"2023-05-30\"]\n"
      ],
      "metadata": {
        "id": "K3YEcNW2RTdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-07-12\",), (\"2023-09-15\",), (\"2023-05-30\",)]\n",
        "\n",
        "schema=[\"booking_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"booking_date\",date_format(col(\"booking_date\"),'dd/MM/yyyy'))\\\n",
        "      .withColumn(\"booking_date\",col(\"booking_date\").cast(\"string\"))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNhph8GdR98o",
        "outputId": "813e4477-859b-4166-b413-0309fb0d824f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|booking_date|\n",
            "+------------+\n",
            "|  2023-07-12|\n",
            "|  2023-09-15|\n",
            "|  2023-05-30|\n",
            "+------------+\n",
            "\n",
            "+------------+\n",
            "|booking_date|\n",
            "+------------+\n",
            "|  12/07/2023|\n",
            "|  15/09/2023|\n",
            "|  30/05/2023|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}