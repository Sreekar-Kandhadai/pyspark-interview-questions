{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1I6URzrCHE+v0/4vBLRF4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreekar-Kandhadai/pyspark-interview-questions/blob/main/Pyspark_Date_Function_Questions_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKFJSRAWvyjg"
      },
      "outputs": [],
      "source": [
        "1. Extracting Year:\n",
        "Given a column event_date (format: yyyy-MM-dd),\n",
        "extract only the year for each row.\n",
        "Sample data:\n",
        "event_date: [\"2023-04-15\",\"2022-11-30\", \"2021-08-25\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[('2023-04-15',),('2022-11-30',), ('2021-08-25',)]\n",
        "\n",
        "schema=['event_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df=df.withColumn(\"event_date\",to_date(col('event_date'),'yyyy-MM-dd'))\n",
        "\n",
        "df1=df.withColumn(\"year\",year(col('event_date')))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trYeq6GDwDfG",
        "outputId": "eaa48f3f-69e1-4772-89c3-25cd194da264"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-04-15|\n",
            "|2022-11-30|\n",
            "|2021-08-25|\n",
            "+----------+\n",
            "\n",
            "+----------+----+\n",
            "|event_date|year|\n",
            "+----------+----+\n",
            "|2023-04-15|2023|\n",
            "|2022-11-30|2022|\n",
            "|2021-08-25|2021|\n",
            "+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2.Date Difference Calculation:\n",
        "Calculate the difference (in days) between two date\n",
        "columns start_date and end_date.\n",
        "Sample data:\n",
        "start_date: [\"2023-01-01\",\"2023-03-15\"]\n",
        "end_date: [\"2023-02-01\",\"2023-03-20\"]\n"
      ],
      "metadata": {
        "id": "WzMoaJqm0S00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-01-01\",\"2023-02-01\"),(\"2023-03-15\",\"2023-03-20\")]\n",
        "\n",
        "schema=[\"start_date\",\"end_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"days\",datediff(col('end_date'),col('start_date')))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odWVNIn_0UII",
        "outputId": "a7ee1f1c-3200-4a43-b5f7-563032a44ea5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|start_date|  end_date|\n",
            "+----------+----------+\n",
            "|2023-01-01|2023-02-01|\n",
            "|2023-03-15|2023-03-20|\n",
            "+----------+----------+\n",
            "\n",
            "+----------+----------+----+\n",
            "|start_date|  end_date|days|\n",
            "+----------+----------+----+\n",
            "|2023-01-01|2023-02-01|  31|\n",
            "|2023-03-15|2023-03-20|   5|\n",
            "+----------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3. Filter Records Based on Date:\n",
        "Filter records where event_date is after 2023-06-01.\n",
        "Sample data:\n",
        "event_date: [\"2023-05-15\",\"2023-07-20\",\"2023-06-05\"]"
      ],
      "metadata": {
        "id": "eX6_aZbW2WFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-05-15\",),(\"2023-07-20\",),(\"2023-06-05\",)]\n",
        "\n",
        "schema=[\"event_date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.filter(\"event_date > '2023-06-01'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPj8GP1c2YxR",
        "outputId": "cc6f7896-d798-4a37-d488-0fe4d4a20339"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-05-15|\n",
            "|2023-07-20|\n",
            "|2023-06-05|\n",
            "+----------+\n",
            "\n",
            "+----------+\n",
            "|event_date|\n",
            "+----------+\n",
            "|2023-07-20|\n",
            "|2023-06-05|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "4. Add 30 days to each date in the order_date column.\n",
        "Sample data:\n",
        "order_date: [\"2023-01-10\", \"2023-06-15\", \"2023-07-30\"]\n"
      ],
      "metadata": {
        "id": "GG0xThFs3cb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-01-10\",), (\"2023-06-15\",), (\"2023-07-30\",)]\n",
        "\n",
        "schema=['order_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn('new_date',date_add(col('order_date'),30))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l991SMUT3hA4",
        "outputId": "9ce49002-5930-42de-8161-e902b1c59c4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|order_date|\n",
            "+----------+\n",
            "|2023-01-10|\n",
            "|2023-06-15|\n",
            "|2023-07-30|\n",
            "+----------+\n",
            "\n",
            "+----------+----------+\n",
            "|order_date|  new_date|\n",
            "+----------+----------+\n",
            "|2023-01-10|2023-02-09|\n",
            "|2023-06-15|2023-07-15|\n",
            "|2023-07-30|2023-08-29|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5. Find the Maximum Date:\n",
        "Determine the latest date from a column\n",
        "payment_date.\n",
        "Sample data:\n",
        "payment_date: [\"2023-02-15\", \"2023-06-25\", \"2023-01-10\"]"
      ],
      "metadata": {
        "id": "ogo1KnUR5S_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data=[(\"2023-02-15\",), (\"2023-06-25\",), (\"2023-01-10\",)]\n",
        "\n",
        "schema=['payment_date']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.select(max('payment_date')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1isg94c55ZMq",
        "outputId": "ead7aad4-25ae-4067-c100-7cc489a3a1ab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|payment_date|\n",
            "+------------+\n",
            "|  2023-02-15|\n",
            "|  2023-06-25|\n",
            "|  2023-01-10|\n",
            "+------------+\n",
            "\n",
            "+-----------------+\n",
            "|max(payment_date)|\n",
            "+-----------------+\n",
            "|       2023-06-25|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}