{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7XNiDidXE0pM8D1Bi05Q+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreekar-Kandhadai/pyspark-interview-questions/blob/main/persistent_systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpwFnPNzHKFh"
      },
      "outputs": [],
      "source": [
        "𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧\n",
        "You are working as a Data Engineer on a big data processing pipeline, where you need to analyze a large dataset from a retail business.\n",
        "This dataset contains transaction information of customers, including a unique transaction ID, customer ID, product ID, transaction amount,\n",
        "and date. The dataset has an index column, but you are required to include this index as a regular column in the DataFrame to perform some\n",
        "further transformations and joins. Your task is to convert the index of the PySpark DataFrame into a column while preserving the dataset's integrity.\n",
        "\n",
        "𝐬𝐜𝐡𝐞𝐦𝐚\n",
        "data = [ (1001, 5001, 200, \"2025-01-01\"),\n",
        "(1002, 5002, 450, \"2025-01-02\"),\n",
        "(1003, 5003, 300, \"2025-01-03\"),\n",
        "(1004, 5004, 150, \"2025-01-04\"),\n",
        "(1005, 5005, 500, \"2025-01-05\"), ]\n",
        "\n",
        "# Schema for DataFrame columns = [\"Customer_ID\", \"Product_ID\", \"Amount\", \"Date\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark=SparkSession.builder.appName(\"learning\").getOrCreate()\n",
        "\n",
        "data= [ (1001, 5001, 200, \"2025-01-01\"),\n",
        "(1002, 5002, 450, \"2025-01-02\"),\n",
        "(1003, 5003, 300, \"2025-01-03\"),\n",
        "(1004, 5004, 150, \"2025-01-04\"),\n",
        "(1005, 5005, 500, \"2025-01-05\"), ]\n",
        "\n",
        "schema= [\"Customer_ID\", \"Product_ID\", \"Amount\", \"Date\"]\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1=df.withColumn(\"index_column\",monotonically_increasing_id())\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBn3oVlvHSN1",
        "outputId": "c28d02ea-28ab-4a93-9ab3-955da2824f1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+------+----------+\n",
            "|Customer_ID|Product_ID|Amount|      Date|\n",
            "+-----------+----------+------+----------+\n",
            "|       1001|      5001|   200|2025-01-01|\n",
            "|       1002|      5002|   450|2025-01-02|\n",
            "|       1003|      5003|   300|2025-01-03|\n",
            "|       1004|      5004|   150|2025-01-04|\n",
            "|       1005|      5005|   500|2025-01-05|\n",
            "+-----------+----------+------+----------+\n",
            "\n",
            "+-----------+----------+------+----------+------------+\n",
            "|Customer_ID|Product_ID|Amount|      Date|index_column|\n",
            "+-----------+----------+------+----------+------------+\n",
            "|       1001|      5001|   200|2025-01-01|           0|\n",
            "|       1002|      5002|   450|2025-01-02|           1|\n",
            "|       1003|      5003|   300|2025-01-03|  8589934592|\n",
            "|       1004|      5004|   150|2025-01-04|  8589934593|\n",
            "|       1005|      5005|   500|2025-01-05|  8589934594|\n",
            "+-----------+----------+------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}